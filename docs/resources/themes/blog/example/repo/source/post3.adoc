= Foundations of Distributed Systems

:Author:        Sofia Andersen
:Category:      Post
:Scope:         Distributed Computing
:Topic:         Architecture
:Status:        Draft
:Priority:      Medium
:Team:          Backend
:Tag:           distributed-systems, cap-theorem, consensus, fault-tolerance
:Public:        No
:Project:       CS Knowledge Base
:Published:     2026-02-18 12:00:00

// END-OF-HEADER. DO NOT MODIFY OR DELETE THIS LINE

== Excerpt

A distributed system is, at its simplest, a collection of independent computers that appear to their users as a single coherent system. That simplicity is deceptive. The moment you introduce a network between components, you inherit an entirely new class of problems: machines fail independently, messages arrive out of order or not at all, and clocks across nodes are never perfectly synchronized. Writing software that handles all of this gracefully is the central challenge of distributed systems engineering.

The field has developed a rich vocabulary for reasoning about these challenges — consensus algorithms, vector clocks, eventual consistency, leader election. These aren't academic curiosities; they're the building blocks of the databases, message queues, and coordination services that modern infrastructure depends on. Before you can use these tools well, you need to understand the failure modes they were designed to address.

== The CAP Theorem

Formulated by Eric Brewer and later proven by Gilbert and Lynch, the CAP theorem states that a distributed data store can provide at most two of the following three guarantees simultaneously:

* *Consistency (C)* — Every read receives the most recent write or an error.
* *Availability (A)* — Every request receives a response (not necessarily the most recent data).
* *Partition Tolerance (P)* — The system continues to operate despite arbitrary network partitions.

Because network partitions are a physical reality in any real distributed deployment, the practical choice is between CP systems (which sacrifice availability during a partition) and AP systems (which sacrifice strong consistency). Understanding this trade-off is fundamental to selecting the right database or messaging technology for a given workload.

== Consistency Models

Beyond the binary framing of CAP, a rich spectrum of consistency models exists:

* *Linearisability* — The strongest model; operations appear instantaneous and globally ordered.
* *Sequential Consistency* — All processes see operations in the same order, but not necessarily in real time.
* *Eventual Consistency* — Given no new updates, all replicas will eventually converge to the same value. Used by systems such as Amazon DynamoDB and Apache Cassandra.
* *Causal Consistency* — Operations that are causally related are seen in causal order by all nodes.

Weaker models allow higher availability and lower latency at the cost of more complex application logic.

== Consensus Algorithms

Many distributed problems reduce to reaching agreement among a set of nodes — electing a leader, committing a transaction, or replicating a log entry. The most influential consensus algorithms include:

* *Paxos* — The classical consensus protocol, known for its theoretical elegance and practical difficulty of implementation.
* *Raft* — Designed explicitly for understandability; it decomposes consensus into leader election, log replication, and safety, making it significantly easier to reason about and implement correctly.
* *PBFT (Practical Byzantine Fault Tolerance)* — Handles _Byzantine_ failures where nodes may behave arbitrarily or maliciously, at the cost of higher message complexity.

== Observability in Distributed Systems

Debugging a monolith is hard; debugging a distributed system is an order of magnitude harder. The three pillars of observability — *logs*, *metrics*, and *traces* — must be designed into a system from the outset. Distributed tracing tools such as Jaeger and Zipkin propagate correlation identifiers across service boundaries, enabling engineers to reconstruct the causal chain of events that led to a failure.

== Conclusion

Distributed systems are both a theoretical discipline and a practical craft. The gap between a system that works on paper and one that survives a production outage is bridged by a deep understanding of failure modes, careful protocol design, and rigorous operational practice.
