= Machine Learning Fundamentals

:Author:        Sofia Andersen
:Category:      Post
:Scope:         Distributed Computing
:Topic:         Machine Learning
:Status:        Draft
:Priority:      Medium
:Team:          Backend
:Tag:           machine-learning, supervised-learning, neural-networks, overfitting, gradient-descent
:Public:        No
:Project:       CS Knowledge Base
:Published:       2026-02-19 14:00:00

// END-OF-HEADER. DO NOT MODIFY OR DELETE THIS LINE

== Excerpt

Machine learning flips the traditional programming model on its head. Instead of writing explicit rules for a computer to follow, you feed it examples and let it infer the rules itself. A spam filter built with ML doesn't need a hand-crafted list of suspicious phrases — it learns statistical patterns from thousands of labeled emails and generalizes from there. This shift in approach has proven extraordinarily powerful across domains that resist manual rule-writing.

At the core of most modern ML systems is a simple optimization loop: make a prediction, measure how wrong it was, and adjust the model's parameters to do better next time. Repeated millions of times across large datasets, this process produces models capable of remarkably sophisticated behavior. The math underneath — linear algebra, calculus, probability — is worth understanding, but even a conceptual grasp of this feedback loop will take you a long way toward reasoning about how these systems behave and where they fail.

== The Learning Paradigm

Traditional software encodes human knowledge as hand-crafted rules: `if temperature > 100 then alert`. Machine learning inverts this: given many examples of inputs paired with desired outputs, an algorithm _infers_ the rules automatically. This is powerful when the rules are too complex to articulate, too numerous to enumerate, or unknown even to domain experts.

The three principal learning paradigms are:

* *Supervised Learning* — The training data consists of labelled input-output pairs. The model learns a mapping from inputs to outputs, then generalises to unseen inputs. Classification and regression are the canonical tasks.
* *Unsupervised Learning* — No labels are provided. The algorithm discovers latent structure in the data — clusters, embeddings, or compressed representations.
* *Reinforcement Learning* — An agent learns by interacting with an environment, receiving reward signals, and adjusting its policy to maximise cumulative reward over time.

== Gradient Descent and Optimisation

Most modern ML models are trained by minimising a _loss function_ that measures the discrepancy between the model's predictions and the ground truth. _Gradient descent_ iteratively adjusts the model's parameters in the direction of steepest descent of the loss landscape.

In practice, _stochastic gradient descent_ (SGD) and its variants — Adam, RMSProp, AdaGrad — compute gradients on small random _mini-batches_ of data rather than the full dataset, making training tractable for large corpora. The _learning rate_ controls the step size at each iteration and is one of the most consequential hyperparameters to tune.

== Neural Networks

Artificial neural networks are a family of models loosely inspired by biological neurons. A network consists of layers of computational units (_neurons_), each computing a weighted sum of its inputs followed by a non-linear _activation function_. _Deep_ networks stack many such layers, enabling them to learn hierarchical representations of data.

Key architectural families include:

* *Convolutional Neural Networks (CNNs)* — Excel at spatial data such as images by applying learned filters across local regions.
* *Recurrent Neural Networks (RNNs) and LSTMs* — Designed for sequential data; maintain a hidden state that captures temporal context.
* *Transformers* — Use _self-attention_ mechanisms to model long-range dependencies without recurrence; now dominant in natural language processing and beyond.

== Overfitting and Regularisation

A model that memorises its training data rather than learning generalisable patterns is said to _overfit_. Symptoms include near-perfect training accuracy paired with poor performance on held-out test data. Regularisation techniques that combat overfitting include L1/L2 weight penalties, dropout (randomly zeroing activations during training), data augmentation, and early stopping.

== Conclusion

Machine learning is not magic — it is calculus, linear algebra, probability theory, and software engineering applied to data. Building intuition for the bias-variance trade-off, the geometry of loss landscapes, and the inductive biases of different model architectures separates practitioners who apply ML effectively from those who apply it blindly.

